{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2 Unnamed: 2  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4  \n",
       "0        NaN        NaN  \n",
       "1        NaN        NaN  \n",
       "2        NaN        NaN  \n",
       "3        NaN        NaN  \n",
       "4        NaN        NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('dataset/spam.csv', encoding='latin-1')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('spam.csv', encoding='latin-1')[['v1', 'v2']] # select these 2 columns only\n",
    "df.columns = ['label', 'message'] # renaming the selected columns\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">message</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>4825</td>\n",
       "      <td>4516</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>747</td>\n",
       "      <td>653</td>\n",
       "      <td>Please call our customer service representativ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      message                                                               \n",
       "        count unique                                                top freq\n",
       "label                                                                       \n",
       "ham      4825   4516                             Sorry, I'll call later   30\n",
       "spam      747    653  Please call our customer service representativ...    4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('label').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2406e2eb278>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(data=df, x='label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets move directly to creating spam filter <br>\n",
    "My approach:<br>\n",
    "1. Clean and Normalize text\n",
    "2. Convert text into vectors (using bag of words model) that machine learning models can understand\n",
    "3. Train and test Classifier\n",
    "4. Clean and normalize text\n",
    "\n",
    "It will be done in following steps:<br>\n",
    "\n",
    "1. Remove punctuations\n",
    "2. Remove all stopwords\n",
    "3. Apply stemming (converting to normal form of word). \n",
    "4. For example, 'driving car' and 'drives car' becomes drive car\n",
    "\n",
    "Note:<br>\n",
    "**Stopwords**: A stop word is a commonly used word (such as “the”, “a”, “an”, “in”) that a search engine has been programmed to ignore, both when indexing entries for searching and when retrieving them as the result of a search query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Saurabh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import PorterStemmer as Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Saurabh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "## Method to return normailzed text in form of tokens (lemmas)\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import PorterStemmer as Stemmer\n",
    "def process(text):\n",
    "    # lowercase it\n",
    "    text = text.lower()\n",
    "    # remove punctuation\n",
    "    text = ''.join([t for t in text if t not in string.punctuation])\n",
    "    # remove stopwords\n",
    "    text = [t for t in text.split() if t not in stopwords.words('english')]\n",
    "    #stemming\n",
    "    st = Stemmer()\n",
    "    text = [st.stem(t) for t in text]\n",
    "    #return token list\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['holiday', 'play', 'cricket', 'jeff', 'play', 'well']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing\n",
    "process('It\\'s holiday and we are playing cricket. Jeff is playing very well!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [go, jurong, point, crazi, avail, bugi, n, gre...\n",
       "1                          [ok, lar, joke, wif, u, oni]\n",
       "2     [free, entri, 2, wkli, comp, win, fa, cup, fin...\n",
       "3         [u, dun, say, earli, hor, u, c, alreadi, say]\n",
       "4     [nah, dont, think, goe, usf, live, around, tho...\n",
       "5     [freemsg, hey, darl, 3, week, word, back, id, ...\n",
       "6     [even, brother, like, speak, treat, like, aid,...\n",
       "7     [per, request, mell, mell, oru, minnaminungint...\n",
       "8     [winner, valu, network, custom, select, receiv...\n",
       "9     [mobil, 11, month, u, r, entitl, updat, latest...\n",
       "10    [im, gonna, home, soon, dont, want, talk, stuf...\n",
       "11    [six, chanc, win, cash, 100, 20000, pound, txt...\n",
       "12    [urgent, 1, week, free, membership, å£100000, ...\n",
       "13    [ive, search, right, word, thank, breather, pr...\n",
       "14                                       [date, sunday]\n",
       "15    [xxxmobilemovieclub, use, credit, click, wap, ...\n",
       "16                                     [oh, kim, watch]\n",
       "17    [eh, u, rememb, 2, spell, name, ye, v, naughti...\n",
       "18    [fine, thatåõ, way, u, feel, thatåõ, way, gota...\n",
       "19    [england, v, macedonia, dont, miss, goalsteam,...\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test with our dataset\n",
    "df['message'][:20].apply(process) # apply process() to message column, ll rows and 20 columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert each message to vectors that machine learning models can understand.\n",
    "We will do that using [**bag-of-words model**](https://machinelearningmastery.com/prepare-text-data-machine-learning-scikit-learn).<br> \n",
    "The model is simple in that it throws away all of the order information in the words and focuses on the occurrence of words in a document.<br>\n",
    "This can be done by assigning each word a unique number. Then any document we see can be encoded as a fixed-length vector with the length of the vocabulary of known words. The value in each position in the vector could be filled with a count or frequency of each word in the encoded document.<br>\n",
    "\n",
    "We will use TfidfVectorizer. It will convert collection of text documents (SMS corpus) into 2D matrix. <br>\n",
    "One dimension represent documents and other dimension repesents each unique word in SMS corpus <br>\n",
    "\n",
    "* One issue with simple counts is that some words like “the” will appear many times and their large counts will not be very meaningful in the encoded vectors. An alternative is to calculate word frequencies, and by far the most popular method is called TF-IDF.\n",
    "\n",
    "* **Term Frequency**: This summarizes how often a given word appears within a document.\n",
    "<h6><center> TF = Number of times term t appears in document (p) / Total number of terms in that document</center></h6>\n",
    "<br>\n",
    "* **Inverse Document Frequency**: is measure of how important term is. For TF, all terms are equally treated. But, in IDF, for words that occur frequently like 'is' 'the' 'of' are assigned less weight. While terms that occur rarely that can easily help identify class of input features will be weighted high.<br>\n",
    "<h6><center>IDF(t)= loge(Total number of documents / Number of documents with term t in it)</center></h6>\n",
    "\n",
    "Without going into the math, TF-IDF are word frequency scores that try to highlight words that are more interesting, e.g. frequent in a document but not across documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# fit and transform\n",
    "tfidfv = TfidfVectorizer(analyzer = process)\n",
    "data = tfidfv.fit_transform(df['message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok lar... Joking wif u oni...\n"
     ]
    }
   ],
   "source": [
    "#check what's in\n",
    "msg = df.iloc[1]['message']\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 7695)\t0.4355907679554564\n",
      "  (0, 7289)\t0.2024772238069804\n",
      "  (0, 5193)\t0.5516408469094822\n",
      "  (0, 5163)\t0.27688324362563543\n",
      "  (0, 4235)\t0.41207318189732706\n",
      "  (0, 4023)\t0.46742635237301533\n"
     ]
    }
   ],
   "source": [
    "output = tfidfv.transform([msg])\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index \t idf \t tfidf \t term\n",
      "4023\t6.7925\t0.4674\tjoke\n",
      "4235\t5.9881\t0.4121\tlar\n",
      "5163\t4.0236\t0.2769\tok\n",
      "5193\t8.0163\t0.5516\toni\n",
      "7289\t2.9423\t0.2025\tu\n",
      "7695\t6.3299\t0.4356\twif\n"
     ]
    }
   ],
   "source": [
    "# A better view\n",
    "\n",
    "j = tfidfv.transform([msg]).toarray()[0]\n",
    "print('index \\t idf \\t tfidf \\t term')\n",
    "for i in range(len(j)):\n",
    "    if j[i] != 0:\n",
    "        print(i, format(tfidfv.idf_[i], '.4f'), format(j[i], '.4f'), tfidfv.get_feature_names()[i],sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having messages in form of vectors, we are ready to train our classifier.<br>\n",
    "We will use Naive Bayes which is well known classifier while working with text data.<br>\n",
    "<br>\n",
    "Before that we will use pipeline feature of sklearn to create a pipeline of TfidfVectorizer followed by Classifier.<br>\n",
    "Input will be message passed to first stage TfidfVectorizer which will transform it and pass it to Naive Bayes Classifier to get output label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "spam_filter = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(analyzer=process)), # messages to weighted TFIDF score\n",
    "    ('classifier', MultinomialNB())                    # train on TFIDF vectors with Naive Bayes\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vectorizer', TfidfVectorizer(analyzer=<function process at 0x000002406E5796A8>,\n",
       "        binary=False, decode_error='strict', dtype=<class 'numpy.float64'>,\n",
       "        encoding='utf-8', input='content', lowercase=True, max_df=1.0,\n",
       "        max_features=None, min_df=1, ngram_range=(1, 1), norm='l...      vocabulary=None)), ('classifier', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perform train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(df['message'], df['label'], test_size=0.20, random_state = 21)\n",
    "\n",
    "# train spam filter\n",
    "spam_filter.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of test cases:  1115\n",
      "Number of wrond predictions:  39\n"
     ]
    }
   ],
   "source": [
    "#predict for test cases\n",
    "predictions = spam_filter.predict(x_test)\n",
    "\n",
    "# count the number of wromng predictons\n",
    "count = 0\n",
    "for i in range(len(y_test)):\n",
    "    if y_test.iloc[i]!= predictions[i]: # iloc is used to retrienve rows from pandas dataset\n",
    "        count+=1\n",
    "        \n",
    "print('Total number of test cases: ', len(y_test))\n",
    "print('Number of wrond predictions: ', count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "419     Send a logo 2 ur lover - 2 names joined by a h...\n",
       "3139    sexy sexy cum and text me im wet and warm and ...\n",
       "3790    Twinks, bears, scallies, skins and jocks are c...\n",
       "2877    Hey Boys. Want hot XXX pics sent direct 2 ur p...\n",
       "2377    YES! The only place in town to meet exciting a...\n",
       "1499    SMS. ac JSco: Energy is high, but u may not kn...\n",
       "3417    LIFE has never been this much fun and great un...\n",
       "3358    Sorry I missed your call let's talk when you h...\n",
       "2412    I don't know u and u don't know me. Send CHAT ...\n",
       "3862    Oh my god! I've found your number again! I'm s...\n",
       "659     88800 and 89034 are premium phone services cal...\n",
       "3109    Good Luck! Draw takes place 28th Feb 06. Good ...\n",
       "5466    http//tms. widelive.com/index. wml?id=820554ad...\n",
       "1268    Can U get 2 phone NOW? I wanna chat 2 set up m...\n",
       "491     Congrats! 1 year special cinema pass for 2 is ...\n",
       "2246    Hi ya babe x u 4goten bout me?' scammers getti...\n",
       "2828    Send a logo 2 ur lover - 2 names joined by a h...\n",
       "3528    Xmas & New Years Eve tickets are now on sale f...\n",
       "4247    accordingly. I repeat, just text the word ok o...\n",
       "4142    In The Simpsons Movie released in July 2007 na...\n",
       "3979                                   ringtoneking 84484\n",
       "1637    0A$NETWORKS allow companies to bill for SMS, s...\n",
       "2802                    FreeMsg>FAV XMAS TONES!Reply REAL\n",
       "3270    You have 1 new voicemail. Please call 08719181...\n",
       "2294     You have 1 new message. Please call 08718738034.\n",
       "2619    <Forwarded from 21870000>Hi - this is your Mai...\n",
       "234     Text & meet someone sexy today. U can find a d...\n",
       "760     Romantic Paris. 2 nights, 2 flights from å£79 ...\n",
       "138     You'll not rcv any more msgs from the chat svc...\n",
       "689     <Forwarded from 448712404000>Please CALL 08712...\n",
       "879     U have a Secret Admirer who is looking 2 make ...\n",
       "1216    You have 1 new voicemail. Please call 08719181...\n",
       "1892    CALL 09090900040 & LISTEN TO EXTREME DIRTY LIV...\n",
       "2351    Download as many ringtones as u like no restri...\n",
       "1317    Win the newest ÛÏHarry Potter and the Order o...\n",
       "4458    Welcome to UK-mobile-date this msg is FREE giv...\n",
       "1879    U have a secret admirer who is looking 2 make ...\n",
       "4309    Someone U know has asked our dating service 2 ...\n",
       "1673    Monthly password for wap. mobsi.com is 391784....\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking from wrong predictons\n",
    "x_test[y_test != predictions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use classification report to get more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       1.00      0.96      0.98      1014\n",
      "        spam       0.72      1.00      0.84       101\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      1115\n",
      "   macro avg       0.86      0.98      0.91      1115\n",
      "weighted avg       0.97      0.97      0.97      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(predictions, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at precision column (for ham, it is 1.00), we can say that all number of wrong predictions (in output above) came from spam predicted as ham. It is ok and cost of predicting spam as ham is negligible to that of predicting ham as spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spam'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to predict whether passed message is ham or spam\n",
    "def detect_spam(s):\n",
    "    return spam_filter.predict([s])[0]\n",
    "\n",
    "detect_spam('Your cash-balance is currently 500 pounds - to maximize your cash-in now, send COLLECT to 83600.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spam'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detect_spam('cash collect')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ham'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detect_spam('cash throw')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
